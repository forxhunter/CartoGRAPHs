{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multidimvis_main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human \n",
    "G = nx.read_edgelist('input/ppi_elist.txt',data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying 1-1000...done.\n",
      "querying 1001-1037...done.\n",
      "Finished.\n",
      "15 input query terms found no hit:\n",
      "\t['nan', 'YGR251W', 'YHR122W', 'TID3', 'YJR141W', 'YLR099W-A', 'MDN1', 'YLR132C', 'YLR243W', 'YMR134W\n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-4543...done.\n",
      "Finished.\n",
      "1 input query terms found dup hits:\n",
      "\t[('IMP2', 2)]\n",
      "311 input query terms found no hit:\n",
      "\t['FLP1', 'REP1', 'RAF1', 'REP2', 'YAL018C', 'YAL044W-A', 'AIM1', 'YAL064C-A', 'YBL010C', 'YBL059W', \n",
      "Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n"
     ]
    }
   ],
   "source": [
    "# Yeast\n",
    "data = pickle.load( open( \"input/BIOGRID-ORGANISM-Saccharomyces_cerevisiae_S288c-3.5.185.mitab.pickle\", \"rb\" ) )\n",
    "\n",
    "Counter(data['Interaction Detection Method'])\n",
    "Counter(data['Interaction Types'])\n",
    "\n",
    "filter_score = data[\n",
    "                    #(data['Interaction Types'] == 'psi-mi:\"MI:0915\"(physical association)') +\n",
    "                    (data['Interaction Types'] == 'psi-mi:\"MI:0407\"(direct interaction)') \n",
    "                    #&\n",
    "                    #(data['Taxid Interactor A'] == \"taxid:559292\") & \n",
    "                    #(data['Taxid Interactor B'] == \"taxid:559292\") \n",
    "]\n",
    "\n",
    "g = nx.from_pandas_edgelist(filter_score, '#ID Interactor A', 'ID Interactor B')\n",
    "g.remove_edges_from(nx.selfloop_edges(g)) #remove self loop\n",
    "\n",
    "G_cere = g.subgraph(max(nx.connected_components(g), key=len)) # largest connected component (lcc)\n",
    "\n",
    "# ESSENTIAL GENES \n",
    "cere_gene =pd.read_csv(\"input/Saccharomyces cerevisiae.csv\",\n",
    "           delimiter= ',',\n",
    "           skipinitialspace=True)\n",
    "essential_cere = cere_gene[(cere_gene['essentiality status'] == 'E')]\n",
    "essential_genes_cere_list =  essential_cere['symbols'].tolist()\n",
    "\n",
    "degree= dict(G_cere.degree())\n",
    "\n",
    "mg = mygene.MyGeneInfo()\n",
    "a = mg.querymany(essential_genes_cere_list, scopes='symbol', species=559292)\n",
    "essential_genes_cere_names = pd.DataFrame.from_dict(a)\n",
    "essential_genes_cere_entrez =  essential_genes_cere_names['entrezgene'].tolist()\n",
    "\n",
    "cleaned_entrez_list = [x for x in essential_genes_cere_entrez if str(x) != 'nan']\n",
    "\n",
    "degree_formatted={}\n",
    "for k, v in degree.items():\n",
    "    degree_formatted[k.replace(\"entrez gene/locuslink:\",\"\")] = v\n",
    "    \n",
    "index= []\n",
    "essential = []\n",
    "for i in cleaned_entrez_list:\n",
    "    for (key, val) in degree_formatted.items():\n",
    "        if i==key:\n",
    "            index.append(key)\n",
    "            essential.append(val)  \n",
    "\n",
    "no_essential_cere = cere_gene[(cere_gene['essentiality status'] == 'NE')]\n",
    "no_essential_genes_cere_list =  no_essential_cere['symbols'].tolist()\n",
    "b = mg.querymany(no_essential_genes_cere_list, scopes='symbol', species=559292)\n",
    "no_essential_genes_cere_names = pd.DataFrame.from_dict(b)\n",
    "no_essential_genes_cere_entrez =  no_essential_genes_cere_names['entrezgene'].tolist()\n",
    "cleaned_entrez_list_no = [x for x in no_essential_genes_cere_entrez if str(x) != 'nan']\n",
    "\n",
    "index= []\n",
    "no_essential = []\n",
    "for i in cleaned_entrez_list_no:\n",
    "    for (key, val) in degree_formatted.items():\n",
    "        if i==key:\n",
    "            index.append(key)\n",
    "            no_essential.append(val)\n",
    "            \n",
    "#df_cere = pd.DataFrame({'essential': pd.Series(essential), 'no_essential': pd.Series(no_essential)})\n",
    "\n",
    "\n",
    "no_ess_id = no_essential_genes_cere_names['entrezgene']\n",
    "ess_id = essential_genes_cere_names['entrezgene']\n",
    "G = G_cere\n",
    "\n",
    "#edge_list = nx.write_edgelist(G, \"Yeast_edgelist_directinteractiononly.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralities of Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 576 ms, sys: 4.72 ms, total: 581 ms\n",
      "Wall time: 582 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# DEGREE CENTRALITY - Node degree: important nodes being involved within high number of interactions\n",
    "degs = dict(G.degree())\n",
    "d_deghubs = {}\n",
    "for node, de in sorted(degs.items(),key = lambda x: x[1], reverse = 1):\n",
    "    d_deghubs[node] = round(float(de/max(degs.values())),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 40s, sys: 3.11 s, total: 11min 43s\n",
      "Wall time: 11min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CLOSENESS CENTRALITY - Measures how closely a node is connected to all other nodes to highlight f.ex. core-periphery structure, or identify central nodes\n",
    "closeness = nx.closeness_centrality(G)\n",
    "d_clos = {}\n",
    "for node, cl in sorted(closeness.items(), key = lambda x: x[1], reverse = 1):\n",
    "    d_clos[node] = round(cl,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 9s, sys: 4.35 s, total: 12min 13s\n",
      "Wall time: 12min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "  \n",
    "# BETWEENESS CENTRALITY - How many shortest paths between pairs of other nodes in the network go through one node. High BC indicates \"bottleneck nodes\" in the network\n",
    "betweens = nx.betweenness_centrality(G)\n",
    "d_betw = {}\n",
    "for node, be in sorted(betweens.items(), key = lambda x: x[1], reverse = 1):\n",
    "     d_betw[node] = round(be,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 24.6 ms, total: 2.56 s\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# EIGENVECTOR CENTRALITY - Compute the eigenvector centrality for the graph\n",
    "eigen = nx.eigenvector_centrality(G)\n",
    "d_eigen = {}\n",
    "for node, eig in sorted(eigen.items(), key = lambda x: x[1], reverse = 1):\n",
    "     d_eigen[node] = round(eig,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_deghubs_sorted = {key:d_deghubs[key] for key in sorted(d_deghubs.keys())}\n",
    "d_clos_sorted = {key:d_clos[key] for key in sorted(d_clos.keys())}\n",
    "d_betw_sorted = {key:d_betw[key] for key in sorted(d_betw.keys())}\n",
    "d_eigen_sorted = {key:d_eigen[key] for key in sorted(d_eigen.keys())}\n",
    "\n",
    "# feature collection\n",
    "feature_dict = dict(zip(d_deghubs_sorted.keys(), zip(\n",
    "                                                     d_deghubs_sorted.values(), \n",
    "                                                     d_clos_sorted.values(), \n",
    "                                                     d_betw_sorted.values(), \n",
    "                                                     d_eigen_sorted.values(),\n",
    "                                                    )))\n",
    "\n",
    "# IMPORTANT :\n",
    "# sort all feature according to Graph node IDs\n",
    "feature_dict_sorted = {key:feature_dict[key] for key in G.nodes()}\n",
    "feature_df = pd.DataFrame.from_dict(feature_dict_sorted, orient = 'index', columns = ['degs', \n",
    "                                                                                      'clos', \n",
    "                                                                                      'betw', \n",
    "                                                                                      'eigen',\n",
    "                                                                                      ]) \n",
    "\n",
    "\n",
    "#l_features = [] \n",
    "#for i in feature_dict_sorted.items():\n",
    "#    k = list(i)\n",
    "#    l_features.append(k)\n",
    "    \n",
    "# feature_df.to_csv(r'output_csv/Features_centralities_Dataframe_'+organism+'.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
